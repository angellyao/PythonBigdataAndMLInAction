{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 分词工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting a.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile a.txt\n",
    "\n",
    "去参观 100 v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.998 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天 我 去 参观 观展 展览 展览馆\n",
      "今天 我 去 参观 展览馆\n",
      "['今天', '我', '去参观', '展览馆']\n",
      "今天 t\n",
      "我 r\n",
      "去参观 v\n",
      "展览馆 n\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "print(' '.join(jieba.cut('今天我去参观展览馆', cut_all=True))) # 全模式\n",
    "print(' '.join(jieba.cut('今天我去参观展览馆', cut_all=False))) # 精确模式\n",
    "\n",
    "jieba.load_userdict('a.txt')\n",
    "print(jieba.lcut('今天我去参观展览馆'))\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"今天我去参观展览馆\")\n",
    "for w in words:\n",
    "    print(\"%s %s\" %(w.word, w.flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 TF-IDF逆文本频率指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['第一天 我 参观 了 美术馆', '第二天 我 参观 了 博物馆', '第三天 我 参观 了 动物园']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "\n",
    "arr = ['第一天我参观了美术馆',\n",
    "       '第二天我参观了博物馆',\n",
    "       '第三天我参观了动物园',]\n",
    "\n",
    "arr = [' '.join(jieba.lcut(i)) for i in arr] # 分词\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   动物园  博物馆  参观  第一天  第三天  第二天  美术馆\n",
      "0    0    0   1    1    0    0    1\n",
      "1    0    1   1    0    0    1    0\n",
      "2    1    0   1    0    1    0    0\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(arr) \n",
    "word = vectorizer.get_feature_names() \n",
    "df = pd.DataFrame(X.toarray(), columns=word)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0句：\n",
      "参观 0.39\n",
      "第一天 0.65\n",
      "美术馆 0.65\n",
      "第1句：\n",
      "博物馆 0.65\n",
      "参观 0.39\n",
      "第二天 0.65\n",
      "第2句：\n",
      "动物园 0.65\n",
      "参观 0.39\n",
      "第三天 0.65\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray()\n",
    "for i in range(len(weight)): # 访问每一句\n",
    "    print(\"第{}句：\".format(i))\n",
    "    for j in range(len(word)):  # 访问每个词\n",
    "        if weight[i][j] > 0.05:  # 只显示重要关键字\n",
    "            print(word[j],round(weight[i][j],2))  # 保留两位小数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'第一天': 1, '我': 1, '参观': 1, '了': 1, '美术馆': 1}), Counter({'第二天': 1, '我': 1, '参观': 1, '了': 1, '博物馆': 1}), Counter({'第三天': 1, '我': 1, '参观': 1, '了': 1, '动物园': 1})]\n",
      "第0句：\n",
      "第一天 0.28\n",
      "我 0.14\n",
      "参观 0.14\n",
      "了 0.14\n",
      "美术馆 0.28\n",
      "第1句：\n",
      "第二天 0.28\n",
      "我 0.14\n",
      "参观 0.14\n",
      "了 0.14\n",
      "博物馆 0.28\n",
      "第2句：\n",
      "第三天 0.28\n",
      "我 0.14\n",
      "参观 0.14\n",
      "了 0.14\n",
      "动物园 0.28\n"
     ]
    }
   ],
   "source": [
    "# 写程序实现TF-IDF方法\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "countlist = []\n",
    "for i in range(len(arr)):\n",
    "    count = Counter(arr[i].split(' ')) # 用空格将字串切分成字符串列表，统计每个词出现次数\n",
    "    countlist.append(count)\n",
    "print(countlist)\n",
    "\n",
    "def tf(word, count): \n",
    "    return count[word] / sum(count.values())\n",
    "def contain(word, count_list): # 统计包含关键词word的句子数量\n",
    "    return sum(1 for count in count_list if word in count)\n",
    "def idf(word, count_list):\n",
    "    return np.log(len(count_list) / (contain(word, count_list)) + 1)  #为避免分母为0，分母加1\n",
    "def tfidf(word, count, count_list):\n",
    "    return tf(word, count) * idf(word, count_list)\n",
    "for i, count in enumerate(countlist):\n",
    "    print(\"第{}句：\".format(i))\n",
    "    scores = {word: tfidf(word, count, countlist) for word in count}\n",
    "    for word, score in scores.items():\n",
    "        print(word, round(score, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
